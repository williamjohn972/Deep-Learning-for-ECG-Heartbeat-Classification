{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "969c0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import F1Score, AUROC, Precision, Recall\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "from utils.torch_classes import ECG_Dataset, EarlyStopping\n",
    "\n",
    "from utils.rnn_models import ECG_LSTM_Classifier, ECG_GRU_Classifier\n",
    "\n",
    "from utils.train import train_and_eval_model, val_loop\n",
    "from utils.logging import log_to_json, log_to_csv\n",
    "\n",
    "from utils.data import calculate_class_weights, calculate_sample_weights\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterSampler, StratifiedKFold\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da9f9b",
   "metadata": {},
   "source": [
    "**TRAINING MODELS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21a97f",
   "metadata": {},
   "source": [
    "-- Setting Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19473ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aee0a2",
   "metadata": {},
   "source": [
    "-- Loading Preprocessed **(Train+Val)** Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53d487a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre processed data\n",
    "data_path = \"data/ecg_preprocessed_train_val.npz\"\n",
    "data = np.load(data_path)\n",
    "\n",
    "X = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "# # assign the weights to all of y_train\n",
    "# train_sample_weights = np.array(class_weights)[y]\n",
    "\n",
    "# train_sample_weights = torch.from_numpy(train_sample_weights).float()\n",
    "\n",
    "# weighted_sampler = WeightedRandomSampler(\n",
    "#     weights= train_sample_weights,\n",
    "#     num_samples= len(train_sample_weights),\n",
    "#     replacement=True,\n",
    "# )\n",
    "\n",
    "# NUM_CLASSES = len(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf49e8a",
   "metadata": {},
   "source": [
    "-- Set up KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "kfold = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f04d8e",
   "metadata": {},
   "source": [
    "-- Set up Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdab14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = 20\n",
    "\n",
    "param_grid = {\n",
    "    \"model\": [\"LSTM\", \"GRU\"],\n",
    "    \"bidirectional\": [True],\n",
    "    \"optimizer\": [\"Adam\", \"AdamW\", \"SGD\"],\n",
    "    \"momentum\": np.linspace(0.9,0.999,3).tolist(),\n",
    "    \"batch_size\": [32,64,128],\n",
    "    \"hidden_size\": [32,64,128],\n",
    "    \"num_layers\": [1,2],\n",
    "    \"dropout\": [0.1,0.2,0.3,0.5],\n",
    "    \"weight_decay\": np.logspace(-5,-2,num=5).tolist(),\n",
    "    \"learning_rate\": np.logspace(-4,-3,num=5).tolist(),\n",
    "}\n",
    "\n",
    "configs = list(ParameterSampler(\n",
    "    param_grid, \n",
    "    n_iter=EXPERIMENTS, \n",
    "    random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "605bc2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'weight_decay': 0.0017782794100389228,\n",
       "  'optimizer': 'AdamW',\n",
       "  'num_layers': 1,\n",
       "  'momentum': 0.9495,\n",
       "  'model': 'GRU',\n",
       "  'learning_rate': 0.0001668100537200059,\n",
       "  'hidden_size': 64,\n",
       "  'dropout': 0.5,\n",
       "  'bidirectional': True,\n",
       "  'batch_size': 128}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print 2 configs to visualise what they look like\n",
    "configs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be246929",
   "metadata": {},
   "source": [
    "-- Random Search + Stratified K Fold:  \n",
    "**To find the best model conifguration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609caad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------Fold 1/5 ---------\n",
      "\n",
      "\n",
      "---------Fold 2/5 ---------\n",
      "\n",
      "\n",
      "---------Fold 3/5 ---------\n",
      "\n",
      "\n",
      "---------Fold 4/5 ---------\n",
      "\n",
      "\n",
      "---------Fold 5/5 ---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LOG_FOLDER = \"random_search_results\"\n",
    "EPOCHS = 30\n",
    "\n",
    "PATIENCE = 5\n",
    "DELTA = 0.0001\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "results_summary_json = []\n",
    "results_summary_csv = []\n",
    "\n",
    "val_precision_metric = Precision(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "val_recall_metric = Recall(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "val_f1_metric = F1Score(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "val_auc_metric = AUROC(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
    "\n",
    "\n",
    "# Random Search Configs\n",
    "for i, params in enumerate(configs):\n",
    "\n",
    "    print(f\"-------------- Experiment {i + 1}/{len(configs)} ----------------\")\n",
    "    try: \n",
    "        \n",
    "        BATCH_SIZE = params['batch_size']\n",
    "        OPTIM = params['optimizer']\n",
    "\n",
    "        INPUT_SIZE = 1\n",
    "        HIDDEN_SIZE = params[\"hidden_size\"]\n",
    "        NUM_LAYERS = params[\"num_layers\"]\n",
    "        DROPOUT = params[\"dropout\"]\n",
    "        BIDIRECTIONAL = params[\"bidirectional\"]\n",
    "\n",
    "        MOMENTUM = params[\"momentum\"]\n",
    "        LEARNING_RATE = params[\"learning_rate\"]\n",
    "        WEIGHT_DECAY = params[\"weight_decay\"]\n",
    "\n",
    "        MODEL = params['model']\n",
    "\n",
    "        fold_metrics = []\n",
    "\n",
    "        print(f\"Model: {'Bi' if BIDIRECTIONAL else ''}{MODEL}\")\n",
    "\n",
    "        # Loop over Each Fold\n",
    "        for fold, (train_index, val_index) in enumerate(kfold.split(X,y)):\n",
    "\n",
    "            # Reset Metrics\n",
    "            for metric in [val_precision_metric,\n",
    "                        val_recall_metric,\n",
    "                        val_f1_metric,\n",
    "                        val_auc_metric]:\n",
    "            \n",
    "                metric.reset()\n",
    "\n",
    "            print(f\"\\n---------Fold {fold+1}/{K} ---------\\n\")\n",
    "\n",
    "            # Split into train and val subsets\n",
    "            X_train_fold, y_train_fold = X[train_index], y[train_index]\n",
    "            X_val_fold, y_val_fold = X[val_index], y[val_index]\n",
    "\n",
    "            train_dataset = ECG_Dataset(X_train_fold, y_train_fold)\n",
    "            val_dataset = ECG_Dataset(X_val_fold,y_val_fold)\n",
    "\n",
    "            # Calculate the class weights \n",
    "            labels, class_weights = calculate_class_weights(y_train_fold)\n",
    "            sample_weights = np.array(class_weights)[y_train_fold]\n",
    "\n",
    "            # Create Weighted Random Sampler\n",
    "            weighted_sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                            num_samples=len(sample_weights),\n",
    "                                            replacement=True)\n",
    "\n",
    "            # We need to recreate the data loaders \n",
    "            train_dataloader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                sampler= weighted_sampler, # this is the Random Weighted Sampler we had created earlier \n",
    "                shuffle= False # we dont need to shuffle because we are using the sampler\n",
    "            )\n",
    "\n",
    "            val_dataloader = DataLoader(\n",
    "                dataset=val_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=False\n",
    "            )\n",
    "\n",
    "            # Create the model\n",
    "            if MODEL == \"LSTM\":\n",
    "                model = ECG_LSTM_Classifier(\n",
    "                    input_size=INPUT_SIZE,\n",
    "                    hidden_size=HIDDEN_SIZE,\n",
    "                    num_layers=NUM_LAYERS,\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=DROPOUT,\n",
    "                    bidirectional=BIDIRECTIONAL\n",
    "                )\n",
    "\n",
    "            elif MODEL == \"GRU\":\n",
    "                model = ECG_GRU_Classifier(\n",
    "                    input_size=INPUT_SIZE,\n",
    "                    hidden_size=HIDDEN_SIZE,\n",
    "                    num_layers=NUM_LAYERS,\n",
    "                    num_classes=NUM_CLASSES,\n",
    "                    dropout=DROPOUT,\n",
    "                    bidirectional=BIDIRECTIONAL\n",
    "                )\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            # Create the optimiers \n",
    "            if OPTIM == \"Adam\":\n",
    "                optim = torch.optim.Adam(params=model.parameters(),\n",
    "                                        lr=LEARNING_RATE,\n",
    "                                        weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "            elif OPTIM == \"AdamW\":\n",
    "                optim = torch.optim.AdamW(params=model.parameters(),\n",
    "                                        lr=LEARNING_RATE,\n",
    "                                        weight_decay=WEIGHT_DECAY)\n",
    "                \n",
    "            elif OPTIM == \"SGD\":\n",
    "                optim = torch.optim.SGD(params=model.parameters(),\n",
    "                                        lr=LEARNING_RATE,\n",
    "                                        momentum=MOMENTUM,\n",
    "                                        weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "            else: \n",
    "                raise ValueError(f\"Unknown optimizer: {OPTIM}\")\n",
    "            \n",
    "            # Create Loss function\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Create Early Stopper\n",
    "            early_stopper = EarlyStopping(\n",
    "                    patience=PATIENCE, \n",
    "                    delta=DELTA,\n",
    "                    checkpoint_path=f\"{LOG_FOLDER}/checkpoints/experiment_{i+1}/fold_{fold+1}.pt\",\n",
    "                    verbose=False)\n",
    "            \n",
    "            # Train the model \n",
    "            start = time.time()\n",
    "            results = train_and_eval_model(\n",
    "                model=model, \n",
    "                loss_fn=loss_func,\n",
    "                optimizer=optim,\n",
    "                device=device,\n",
    "\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                \n",
    "                epochs=EPOCHS, # small epoch number during this phase\n",
    "                early_stopper= early_stopper,\n",
    "\n",
    "                verbose=False,\n",
    "                debug=False,\n",
    "\n",
    "                grad_clip=True,\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "            epochs_run = len(results[\"train_loss\"])\n",
    "\n",
    "            total_time = (end-start)\n",
    "            time_per_epoch = total_time/epochs_run if epochs_run > 0 else 0 # (i+1) is the current epoch\n",
    "\n",
    "            # Evaluate the best checkpoint\n",
    "            model.load_state_dict(torch.load(early_stopper.checkpoint_path, map_location=device)[\"model_state_dict\"])\n",
    "            val_data = val_loop(model=model, \n",
    "                                    val_dataloader=val_dataloader,\n",
    "                                    loss_fn=loss_func, \n",
    "                                    device=device)\n",
    "            \n",
    "            val_pred = torch.cat(val_data[\"y_pred\"])\n",
    "            val_true = torch.cat(val_data[\"y_true\"])\n",
    "\n",
    "            val_pred_logits = torch.cat(val_data[\"y_pred_logits\"])\n",
    "\n",
    "            # Calculate Precision, Recall, F1 and AUC\n",
    "            fold_precision = val_precision_metric(val_pred.to(device), val_true.to(device)).item()\n",
    "            fold_recall = val_recall_metric(val_pred.to(device), val_true.to(device)).item()\n",
    "            fold_f1 = val_f1_metric(val_pred.to(device), val_true.to(device)).item()\n",
    "            fold_auc = val_auc_metric(val_pred_logits.to(device), val_true.to(device)).item()\n",
    "\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold+1,\n",
    "                \"precision\": fold_precision,\n",
    "                \"recall\": fold_recall,\n",
    "                \"f1\": fold_f1,\n",
    "                \"auc\": fold_auc,\n",
    "                \"time_per_epoch\": time_per_epoch, \n",
    "            })\n",
    "\n",
    "            print(f\"Fold {fold+1}: F1={fold_f1:.3f} | AUC={fold_auc:.3f}\")\n",
    "\n",
    "        # Agreggrate metrics across folds\n",
    "        avg_recall = np.mean([m[\"recall\"] for m in fold_metrics])\n",
    "        avg_precision = np.mean([m[\"precision\"] for m in fold_metrics])\n",
    "        avg_f1 = np.mean([m[\"f1\"] for m in fold_metrics])\n",
    "        avg_auc = np.mean([m[\"auc\"] for m in fold_metrics])\n",
    "\n",
    "        avg_time_per_epoch = np.mean(m[\"time_per_epoch\"] for m in fold_metrics)\n",
    "\n",
    "        results_summary_csv.append({\n",
    "            \"experiment\": i+1,\n",
    "            \"avg_precision\": avg_precision,\n",
    "            \"avg_recall\": avg_recall,\n",
    "            \"avg_f1\": avg_f1,\n",
    "            \"avg_auc\": avg_auc,\n",
    "            \"avg_time_per_epoch\": avg_time_per_epoch\n",
    "        })\n",
    "\n",
    "        if OPTIM != \"SGD\":\n",
    "            params[\"momentum\"] = None\n",
    "\n",
    "        results_summary_json.append({\n",
    "            \"experiment\": i+1,\n",
    "            **params,\n",
    "            \"fold_metrics\": fold_metrics\n",
    "        })\n",
    "\n",
    "        log_to_csv(f\"{LOG_FOLDER}/results.csv\", results_summary_csv)\n",
    "        log_to_json(f\"{LOG_FOLDER}/results.json\", results_summary_json)\n",
    "\n",
    "        print(f\"\\n Experiment {i+1} Done: Avg F1={avg_f1:.3f}, Avg AUC={avg_auc:.3f}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Experiment {i+1} failed: {e}\")\n",
    "\n",
    "print(\"\\n---------------- All Experiments Completed ----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fa8be2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.12)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_metrics = [\n",
    "    {\"f1\": 0.11},\n",
    "    {\"f1\": 0.12},\n",
    "    {\"f1\": 0.13},\n",
    "]\n",
    "\n",
    "np.mean([m[\"f1\"] for m in fold_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0131608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
