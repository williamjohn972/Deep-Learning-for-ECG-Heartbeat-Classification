{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN Model Training Notebook\n",
        "\n",
        "This notebook mirrors the experimental workflow of the RNN notebook. It performs hyper-parameter searches, model training, and final evaluation for the CNN architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchmetrics import AUROC, F1Score, Precision, Recall\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import ParameterSampler, StratifiedKFold, train_test_split\n",
        "\n",
        "from utils.cnn_models import ECG_CNN_Classifier\n",
        "from utils.data import calculate_class_weights, split_x_y\n",
        "from utils.logging import log_to_csv, log_to_json\n",
        "from utils.preprocessing import Preprocessing\n",
        "from utils.torch_classes import ECG_Dataset, EarlyStopping\n",
        "from utils.train import test_loop, train_and_eval_model, val_loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ensure reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load preprocessed training and validation data\n",
        "train_val_path = \"data/ecg_preprocessed_train_val.npz\"\n",
        "train_val_data = np.load(train_val_path)\n",
        "\n",
        "X = train_val_data[\"X\"]\n",
        "y = train_val_data[\"y\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Stratified K-Fold setup\n",
        "K = 5\n",
        "kfold = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Hyper-parameter search space for the CNN experiments\n",
        "EXPERIMENTS = 20\n",
        "\n",
        "param_grid = {\n",
        "    \"conv_channels\": [\n",
        "        (32, 64, 128),\n",
        "        (64, 128, 256),\n",
        "        (32, 64, 64),\n",
        "        (64, 64, 128),\n",
        "    ],\n",
        "    \"kernel_sizes\": [\n",
        "        (7, 5, 3),\n",
        "        (9, 7, 5),\n",
        "        (5, 5, 3),\n",
        "    ],\n",
        "    \"pool_kernel_sizes\": [\n",
        "        (2, 2, 2),\n",
        "        (2, 2, 1),\n",
        "    ],\n",
        "    \"dropout\": [0.1, 0.2, 0.3, 0.4],\n",
        "    \"fc_hidden_dim\": [128, 256, None],\n",
        "    \"use_batch_norm\": [True, False],\n",
        "    \"optimizer\": [\"Adam\", \"AdamW\", \"SGD\"],\n",
        "    \"momentum\": np.linspace(0.85, 0.95, 3).tolist(),\n",
        "    \"batch_size\": [64, 128, 256],\n",
        "    \"learning_rate\": np.logspace(-4, -3, num=5).tolist(),\n",
        "    \"weight_decay\": np.logspace(-5, -3, num=5).tolist(),\n",
        "}\n",
        "\n",
        "configs = list(\n",
        "    ParameterSampler(\n",
        "        param_grid=param_grid,\n",
        "        n_iter=EXPERIMENTS,\n",
        "        random_state=42,\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Preview one sampled configuration\n",
        "configs[:1]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "LOG_FOLDER = \"cnn_random_search\"\n",
        "EPOCHS = 30\n",
        "\n",
        "PATIENCE = 6\n",
        "DELTA = 1e-4\n",
        "\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "results_summary_json = []\n",
        "results_summary_csv = []\n",
        "\n",
        "for i, params in enumerate(configs, start=1):\n",
        "    print(f\"-------------- Experiment {i}/{len(configs)} ----------------\")\n",
        "    try:\n",
        "        params_copy = dict(params)\n",
        "\n",
        "        batch_size = params_copy[\"batch_size\"]\n",
        "        optimizer_name = params_copy[\"optimizer\"]\n",
        "        conv_channels = params_copy[\"conv_channels\"]\n",
        "        kernel_sizes = params_copy[\"kernel_sizes\"]\n",
        "        pool_kernel_sizes = params_copy[\"pool_kernel_sizes\"]\n",
        "        dropout = params_copy[\"dropout\"]\n",
        "        fc_hidden_dim = params_copy[\"fc_hidden_dim\"]\n",
        "        use_batch_norm = params_copy[\"use_batch_norm\"]\n",
        "\n",
        "        momentum = params_copy[\"momentum\"]\n",
        "        learning_rate = params_copy[\"learning_rate\"]\n",
        "        weight_decay = params_copy[\"weight_decay\"]\n",
        "\n",
        "        fold_metrics = []\n",
        "\n",
        "        for fold, (train_index, val_index) in enumerate(kfold.split(X, y), start=1):\n",
        "            print(f\"\n",
        "--------- Fold {fold}/{K} ---------\n",
        "\")\n",
        "\n",
        "            X_train_fold, y_train_fold = X[train_index], y[train_index]\n",
        "            X_val_fold, y_val_fold = X[val_index], y[val_index]\n",
        "\n",
        "            train_dataset = ECG_Dataset(X_train_fold, y_train_fold)\n",
        "            val_dataset = ECG_Dataset(X_val_fold, y_val_fold)\n",
        "\n",
        "            _, class_weights = calculate_class_weights(y_train_fold)\n",
        "            sample_weights = np.array(class_weights)[y_train_fold]\n",
        "            weighted_sampler = WeightedRandomSampler(\n",
        "                weights=sample_weights,\n",
        "                num_samples=len(sample_weights),\n",
        "                replacement=True,\n",
        "            )\n",
        "\n",
        "            train_dataloader = DataLoader(\n",
        "                dataset=train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                sampler=weighted_sampler,\n",
        "                shuffle=False,\n",
        "                num_workers=2,\n",
        "                persistent_workers=True,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            val_dataloader = DataLoader(\n",
        "                dataset=val_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=2,\n",
        "                persistent_workers=True,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            model = ECG_CNN_Classifier(\n",
        "                num_classes=NUM_CLASSES,\n",
        "                in_channels=1,\n",
        "                conv_channels=conv_channels,\n",
        "                kernel_sizes=kernel_sizes,\n",
        "                pool_kernel_sizes=pool_kernel_sizes,\n",
        "                dropout=dropout,\n",
        "                fc_hidden_dim=fc_hidden_dim,\n",
        "                use_batch_norm=use_batch_norm,\n",
        "            )\n",
        "            model.to(device)\n",
        "\n",
        "            if optimizer_name == \"Adam\":\n",
        "                optimizer = torch.optim.Adam(\n",
        "                    params=model.parameters(),\n",
        "                    lr=learning_rate,\n",
        "                    weight_decay=weight_decay,\n",
        "                )\n",
        "            elif optimizer_name == \"AdamW\":\n",
        "                optimizer = torch.optim.AdamW(\n",
        "                    params=model.parameters(),\n",
        "                    lr=learning_rate,\n",
        "                    weight_decay=weight_decay,\n",
        "                )\n",
        "            elif optimizer_name == \"SGD\":\n",
        "                optimizer = torch.optim.SGD(\n",
        "                    params=model.parameters(),\n",
        "                    lr=learning_rate,\n",
        "                    momentum=momentum,\n",
        "                    weight_decay=weight_decay,\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
        "\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            early_stopper = EarlyStopping(\n",
        "                patience=PATIENCE,\n",
        "                delta=DELTA,\n",
        "                checkpoint_path=f\"{LOG_FOLDER}/checkpoints/experiment_{i}/fold_{fold}.pt\",\n",
        "                verbose=True,\n",
        "            )\n",
        "\n",
        "            precision_metric = Precision(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
        "            recall_metric = Recall(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
        "            f1_metric = F1Score(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
        "            auc_metric = AUROC(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").to(device)\n",
        "\n",
        "            start = time.time()\n",
        "            history = train_and_eval_model(\n",
        "                model=model,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                train_dataloader=train_dataloader,\n",
        "                val_dataloader=val_dataloader,\n",
        "                epochs=EPOCHS,\n",
        "                device=device,\n",
        "                early_stopper=early_stopper,\n",
        "                debug=True,\n",
        "                verbose=True,\n",
        "                grad_clip=True,\n",
        "                max_norm=1.0,\n",
        "            )\n",
        "            end = time.time()\n",
        "\n",
        "            epochs_run = len(history[\"train_loss\"])\n",
        "            total_time = end - start\n",
        "            time_per_epoch = total_time / epochs_run if epochs_run > 0 else 0.0\n",
        "\n",
        "            checkpoint = torch.load(early_stopper.checkpoint_path, map_location=device)\n",
        "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "            val_data = val_loop(\n",
        "                model=model,\n",
        "                val_dataloader=val_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "            val_pred = torch.cat(val_data[\"y_pred\"])\n",
        "            val_true = torch.cat(val_data[\"y_true\"])\n",
        "            val_logits = torch.cat(val_data[\"y_pred_logits\"])\n",
        "\n",
        "            fold_precision = precision_metric(val_pred.to(device), val_true.to(device)).item()\n",
        "            fold_recall = recall_metric(val_pred.to(device), val_true.to(device)).item()\n",
        "            fold_f1 = f1_metric(val_pred.to(device), val_true.to(device)).item()\n",
        "            fold_auc = auc_metric(val_logits.to(device), val_true.to(device)).item()\n",
        "\n",
        "            fold_metrics.append(\n",
        "                {\n",
        "                    \"fold\": fold,\n",
        "                    \"precision\": fold_precision,\n",
        "                    \"recall\": fold_recall,\n",
        "                    \"f1\": fold_f1,\n",
        "                    \"auc\": fold_auc,\n",
        "                    \"time_per_epoch\": time_per_epoch,\n",
        "                    \"epochs_run\": epochs_run,\n",
        "                    \"total_epochs\": EPOCHS,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            print(f\"Fold {fold}: F1={fold_f1:.3f} | AUC={fold_auc:.3f}\")\n",
        "\n",
        "        avg_precision = np.mean([m[\"precision\"] for m in fold_metrics])\n",
        "        avg_recall = np.mean([m[\"recall\"] for m in fold_metrics])\n",
        "        avg_f1 = np.mean([m[\"f1\"] for m in fold_metrics])\n",
        "        avg_auc = np.mean([m[\"auc\"] for m in fold_metrics])\n",
        "        avg_time_per_epoch = np.mean([m[\"time_per_epoch\"] for m in fold_metrics])\n",
        "        avg_epochs_run = np.mean([m[\"epochs_run\"] for m in fold_metrics])\n",
        "\n",
        "        results_summary_csv.append(\n",
        "            {\n",
        "                \"experiment\": i,\n",
        "                \"avg_precision\": avg_precision,\n",
        "                \"avg_recall\": avg_recall,\n",
        "                \"avg_f1\": avg_f1,\n",
        "                \"avg_auc\": avg_auc,\n",
        "                \"avg_time_per_epoch\": avg_time_per_epoch,\n",
        "                \"avg_epochs_run\": avg_epochs_run,\n",
        "                \"total_epochs\": EPOCHS,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if optimizer_name != \"SGD\":\n",
        "            params_copy[\"momentum\"] = None\n",
        "\n",
        "        results_summary_json.append(\n",
        "            {\n",
        "                \"experiment\": i,\n",
        "                **params_copy,\n",
        "                \"fold_metrics\": fold_metrics,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        log_to_csv(f\"{LOG_FOLDER}/results.csv\", results_summary_csv)\n",
        "        log_to_json(f\"{LOG_FOLDER}/results.json\", results_summary_json)\n",
        "\n",
        "        print(f\"\n",
        "Experiment {i} Done: Avg F1={avg_f1:.3f}, Avg AUC={avg_auc:.3f}\n",
        "\")\n",
        "\n",
        "    except Exception as exc:\n",
        "        print(f\"Experiment {i} failed: {exc}\")\n",
        "\n",
        "print(\"\n",
        "---------------- All Experiments Completed ----------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Final CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a dedicated train/validation split for the final model\n",
        "train_val_split = 0.05\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=train_val_split,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "train_dataset = ECG_Dataset(X_train, y_train)\n",
        "val_dataset = ECG_Dataset(X_val, y_val)\n",
        "\n",
        "_, class_weights = calculate_class_weights(y_train)\n",
        "train_sample_weights = np.array(class_weights)[y_train]\n",
        "\n",
        "weighted_sampler = WeightedRandomSampler(\n",
        "    weights=train_sample_weights,\n",
        "    num_samples=len(train_sample_weights),\n",
        "    replacement=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Selected hyper-parameters from the search\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 5e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "CONV_CHANNELS = (64, 128, 256)\n",
        "KERNEL_SIZES = (7, 5, 3)\n",
        "POOL_KERNEL_SIZES = (2, 2, 2)\n",
        "DROPOUT = 0.2\n",
        "FC_HIDDEN_DIM = 256\n",
        "USE_BATCH_NORM = True\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "PATIENCE = 12\n",
        "DELTA = 1e-4\n",
        "CHECKPOINT_PATH = \"models/best_CNN.pt\"\n",
        "\n",
        "LR_PATIENCE = 6\n",
        "MIN_LR = 1e-4\n",
        "FACTOR = 0.5\n",
        "\n",
        "GRAD_CLIP = True\n",
        "MAX_NORM = 1.0\n",
        "\n",
        "EPOCHS = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = ECG_CNN_Classifier(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    in_channels=1,\n",
        "    conv_channels=CONV_CHANNELS,\n",
        "    kernel_sizes=KERNEL_SIZES,\n",
        "    pool_kernel_sizes=POOL_KERNEL_SIZES,\n",
        "    dropout=DROPOUT,\n",
        "    fc_hidden_dim=FC_HIDDEN_DIM,\n",
        "    use_batch_norm=USE_BATCH_NORM,\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer=optimizer,\n",
        "    mode=\"min\",\n",
        "    factor=FACTOR,\n",
        "    patience=LR_PATIENCE,\n",
        "    min_lr=MIN_LR,\n",
        ")\n",
        "\n",
        "early_stopper = EarlyStopping(\n",
        "    patience=PATIENCE,\n",
        "    delta=DELTA,\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=weighted_sampler,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "train_history = train_and_eval_model(\n",
        "    model=model,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=EPOCHS,\n",
        "    device=device,\n",
        "    early_stopper=early_stopper,\n",
        "    scheduler=lr_scheduler,\n",
        "    debug=True,\n",
        "    verbose=True,\n",
        "    grad_clip=GRAD_CLIP,\n",
        "    max_norm=MAX_NORM,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
        "plt.plot(train_history[\"val_loss\"], label=\"Validation Loss\", marker=\"s\")\n",
        "\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Prepare the test dataset\n",
        "TEST_DATA_PATH = \"data/mitbih_test.csv\"\n",
        "test_df = pd.read_csv(TEST_DATA_PATH)\n",
        "X_test, y_test = split_x_y(test_df)\n",
        "\n",
        "preprocess = Preprocessing(\n",
        "    sample_freq=125,\n",
        "    cutoff_freq=25,\n",
        "    order=3,\n",
        "    target_r_peak_index=94,\n",
        "    method=\"neurokit\",\n",
        ")\n",
        "\n",
        "X_test_preprocessed = preprocess.transform(X_test)\n",
        "\n",
        "test_dataset = ECG_Dataset(X_test_preprocessed, y_test)\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    pin_memory=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device)[\"model_state_dict\"])\n",
        "model.to(device)\n",
        "\n",
        "test_results = test_loop(model=model, test_dataloader=test_dataloader, device=device)\n",
        "\n",
        "y_pred = test_results[\"y_pred\"]\n",
        "y_true = test_results[\"y_true\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "labels_list = [\"N\", \"S\", \"V\", \"F\", \"Q\"]\n",
        "print(\n",
        "    classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        target_names=labels_list,\n",
        "        digits=4,\n",
        "    )\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}